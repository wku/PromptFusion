# Процесс работы с промпт-инструментами

## Автоматическое извлечение информации для ответов

Промпт-инструменты позволяют LLM самостоятельно запрашивать необходимые данные из проекта для ответа на вопросы. Вот как это работает:

### Пример автоматического использования get_file

1. **Пользователь задает вопрос:**
   "Какие функции есть в main.py и что они делают?"

2. **LLM распознает необходимость получить содержимое файла:**
   ```
   Для ответа мне необходимо изучить файл main.py
   [FUNCTION: get_file(path="./main.py")]
   ```

3. **Система обрабатывает вызов функции:**
   - Извлекает путь к файлу
   - Загружает содержимое файла
   - Форматирует результат

4. **Результат включается в контекст:**
   ```
   [RESULT: get_file]
   Файл: ./main.py
   Содержимое:
   ```
   def initialize_project():
       print("Инициализация проекта")
   
   def analyze_files():
       print("Анализ файлов")
   ```
   [/RESULT]
   ```

5. **LLM продолжает ответ, используя полученную информацию:**
   "В файле main.py определены следующие функции:
   - `initialize_project()` - отвечает за инициализацию проекта
   - `analyze_files()` - выполняет анализ файлов"

### Использование семантического поиска

Когда пользователь задает вопрос о конкретной функциональности, но не указывает файл:

1. **Пользователь задает вопрос:**
   "Как реализован анализ структуры файлов проекта?"

2. **LLM использует семантический поиск:**
   ```
   Давайте найдем файлы, связанные с анализом структуры
   [FUNCTION: find_files_semantic(query="анализ структуры файлов", page=0)]
   ```

3. **Система возвращает список релевантных файлов:**
   ```
   [RESULT: find_files_semantic]
   Запрос: анализ структуры файлов
   Страница: 0
   Найденные файлы:
   - ./analyzers.py
   - ./utils.py
   - ./core.py
   [/RESULT]
   ```

4. **LLM может затем запросить содержимое конкретного файла:**
   ```
   Проверим содержимое найденного файла
   [FUNCTION: get_file(path="./analyzers.py")]
   ```

### Поиск по содержимому файлов

При поиске конкретного кода или определений:

1. **Пользователь спрашивает:**
   "Где определена функция process_data?"

2. **LLM выполняет поиск по содержимому:**
   ```
   Найдем определение функции
   [FUNCTION: find_in_files(query="def process_data", is_case_sensitive=false, page=0)]
   ```

3. **Система находит все вхождения:**
   ```
   [RESULT: find_in_files]
   Запрос: def process_data
   Учет регистра: Нет
   Страница: 0
   
   Файл: ./utils.py
     45  def process_data(data, options=None):
     46      """Обрабатывает данные с учетом опций"""
     47      results = []
   
   Файл: ./core.py
     218  def process_data(data, remove_comments=False):
     219      """Обрабатывает данные файлов проекта"""
     220      if remove_comments:
   [/RESULT]
   ```

### Обновление и создание файлов

Когда требуется исправить ошибку или создать новый файл:

1. **Пользователь просит внести изменения:**
   "Исправь ошибку деления на ноль в t.py"

2. **LLM сначала получает содержимое файла:**
   ```
   [FUNCTION: get_file(path="./t.py")]
   ```

3. **После анализа файла LLM предлагает исправление:**
   ```
   Необходимо исправить функцию divide:
   [FUNCTION: update_file(path="./t.py", content="def divide(a, b):\n    if b == 0:\n        return 'Ошибка: деление на ноль'\n    return a / b\n\nresult = divide(2, 1)  # Исправлено")]
   ```

4. **Система запрашивает подтверждение у пользователя:**
   ```
   Модель хочет обновить файл: ./t.py, подтверждаете? [Y/n]:
   ```

5. **После подтверждения система обновляет файл и возвращает результат:**
   ```
   [RESULT: update_file]
   Файл: ./t.py
   Статус: Файл обновлен
   [/RESULT]
   ```

## Особенности работы с промпт-инструментами

- **Прозрачность процесса:** Все действия видны в истории диалога
- **Интерактивное подтверждение:** Модификация файлов требует явного подтверждения пользователя
- **Последовательность обработки:** Система обрабатывает вызовы функций один за другим
- **Контекстная память:** Результаты функций остаются в контексте диалога
- **Автоматический выбор файлов:** LLM самостоятельно определяет, какие файлы нужны для ответа

Данный подход обеспечивает гибкое и интуитивное взаимодействие с кодовой базой, позволяя LLM самостоятельно запрашивать необходимую информацию и предлагать конкретные изменения в файлах.


